{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Most Code here\n",
    "import sys\n",
    "import scipy.misc, scipy.ndimage.interpolation\n",
    "import pickle\n",
    "sys.path.append(\"/opt/saratan/data/layers\")\n",
    "sys.path.append(\"/opt/saratan/\")\n",
    "\n",
    "import plyvel, saratan_utils, math, re, time\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "plt.set_cmap('gray')\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "import pandas\n",
    "from PIL import Image,ImageFilter\n",
    "\n",
    "# ADD VNET STUFF\n",
    "import VNet as VN\n",
    "import utilities\n",
    "import DataManager as DM\n",
    "import pyLayer\n",
    "import main\n",
    "import os\n",
    "from multiprocessing import Process, Queue\n",
    "from os.path import splitext, join\n",
    "import re\n",
    "import SimpleITK as sitk\n",
    "\n",
    "\n",
    "def hist(arr):\n",
    "    \"\"\"Print number of pixels for each label in the given image (arr)\"\"\"\n",
    "    return \"%.3f , %.3f , %.3f, %.3f\" % (np.sum(arr==0),np.sum(arr==1),np.sum(arr==2),np.sum(arr==4))\n",
    "\n",
    "def imshow(*args,**kwargs):\n",
    "    \"\"\" Handy function to show multiple plots in on row, possibly with different cmaps and titles\n",
    "    Usage: \n",
    "    imshow(img1, title=\"myPlot\")\n",
    "    imshow(img1,img2, title=['title1','title2'])\n",
    "    imshow(img1,img2, cmap='hot')\n",
    "    imshow(img1,img2,cmap=['gray','Blues']) \"\"\"\n",
    "    cmap = kwargs.get('cmap', 'gray')\n",
    "    title= kwargs.get('title','')\n",
    "    axis_enabled = kwargs.get('axis',True)\n",
    "\n",
    "    if len(args)==0:\n",
    "        raise ValueError(\"No images given to imshow\")\n",
    "    elif len(args)==1:\n",
    "        if not axis_enabled:\n",
    "            plt.axis('off')\n",
    "        plt.title(title)\n",
    "        plt.imshow(args[0], interpolation='none')\n",
    "    else:\n",
    "        n=len(args)\n",
    "        if type(cmap)==str:\n",
    "            cmap = [cmap]*n\n",
    "        if type(title)==str:\n",
    "            title= [title]*n\n",
    "        plt.figure(figsize=(n*5,10))\n",
    "        for i in range(n):\n",
    "            plt.subplot(1,n,i+1)\n",
    "            plt.title(title[i])\n",
    "            if not axis_enabled:\n",
    "                plt.axis('off')\n",
    "            plt.imshow(args[i], cmap[i], interpolation='none')\n",
    "    plt.show()\n",
    "        \n",
    "def dice(prediction, segmentation, label_of_interest = 1):\n",
    "    \"\"\" Takes 2 2-D arrays with class labels, and return a float dice score.\n",
    "    Only label=label_of_interest is considered \"\"\"\n",
    "    if prediction.shape != segmentation.shape:\n",
    "        raise ValueError(\"Shape mismatch between given arrays. prediction %s vs segmentation %s\" \\\n",
    "                         % (str(prediction.shape), str(segmentation.shape)))\n",
    "\n",
    "    n_liver_seg = np.sum(segmentation==label_of_interest)\n",
    "    n_liver_pred= np.sum(prediction == label_of_interest)\n",
    "    denominator = n_liver_pred + n_liver_seg\n",
    "    if denominator == 0:\n",
    "        return -1\n",
    "\n",
    "    liver_intersection   = np.logical_and(prediction==label_of_interest, segmentation==label_of_interest)\n",
    "    n_liver_intersection = np.sum(liver_intersection)\n",
    "\n",
    "    dice_score = 2.0*n_liver_intersection / denominator\n",
    "    return dice_score\n",
    "\n",
    "    \n",
    "def protobinary_to_array(filename, outpng=None):\n",
    "    \"\"\" Filename is path to protobinary\n",
    "    outpng is path to output png\"\"\"\n",
    "    with open(filename,'r') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    blob = caffe.proto.caffe_pb2.BlobProto()\n",
    "    blob.ParseFromString(data)\n",
    "    arr = np.array(caffe.io.blobproto_to_array(blob)) #returns shape (1,1,W,H)\n",
    "    arr = arr[0,0,:,:] #ignore first 2 dimensions\n",
    "    return  arr\n",
    "\n",
    "dices_liver = []\n",
    "dices_lesion= []\n",
    "def predict(net, img, seg, meanimg):\n",
    "    \"\"\"Predicts an img using the trained net, and compares it to the label image (seg)\"\"\"\n",
    "    net.blobs['data'].data[0]=(img-meanimg)\n",
    "    prob=net.forward()['prob'][0]\n",
    "    prediction = np.argmax(prob,axis=0)\n",
    "    dice_liver = dice(prediction,seg,label_of_interest=1)\n",
    "    dice_lesion = dice(prediction,seg,label_of_interest=2)\n",
    "    dices_liver.append(dice_liver)\n",
    "    dices_lesion.append(dice_lesion)\n",
    "    print \"Dice Liver:\", dice_liver\n",
    "    print \"Dice Lesion:\",dice_lesion\n",
    "    print \"Prediction class histogram\",hist(prediction)\n",
    "    print \"Ground truth class histogram\",hist(seg)\n",
    "    plt.figure(figsize=(20,24))\n",
    "    plt.subplot(1,3,1); plt.title(\"Image\")\n",
    "    plt.imshow(img)\n",
    "    plt.subplot(1,3,2); plt.title(\"Ground truth\")\n",
    "    plt.imshow(seg)\n",
    "    plt.subplot(1,3,3); plt.title(\"Prediction\")\n",
    "    plt.imshow(prediction)\n",
    "    plt.show()\n",
    "    \n",
    "def read_imgs(dbimgit, dbsegit, n=1, print_keys=True):\n",
    "    \"\"\"Read img and label after skipping n keys in leveldb. Takes db iterators\"\"\"\n",
    "    for _ in range(n):\n",
    "        k1,vimg = dbimgit.next()\n",
    "        k2,vseg = dbsegit.next()\n",
    "    if print_keys:\n",
    "        print \"Keys:\",k1,k2\n",
    "    img=lutils.to_numpy_matrix(vimg)\n",
    "    seg=lutils.to_numpy_matrix(vseg)\n",
    "    return img,seg\n",
    "\n",
    "def show_kernels(layer_blob_data, fast = False):\n",
    "    \"\"\" Takes solver.net.params['conv1'][0].data and visualize the first channel of all kernels.\n",
    "    If fast = False : subplots will be used, allowing to see each filter individually, but takes time.\n",
    "    If fast = True : all filters are plotted in one image\"\"\"\n",
    "    #Input has 4 dims, we only visualize 1st channel of each kernel \n",
    "    # (the conv weights that acts on the 1st channel of the input)\n",
    "    data = layer_blob_data[:,0,:,:]\n",
    "    if fast:\n",
    "        raise NotImplementedError(\"todo\")\n",
    "    \n",
    "    # Sort\n",
    "    sorted_data = sorted(data, key=lambda x: np.sum(x))\n",
    "    data = np.array(sorted_data)\n",
    "    \n",
    "    n_kernels = np.array(data).shape[0]\n",
    "    plot_cols = 20 #number of images in one row\n",
    "    plot_rows = math.ceil(n_kernels*1.0 / plot_cols)\n",
    "    # Adjust figure plot size\n",
    "    plt.figure(figsize=(min(plot_cols, n_kernels)*0.7, plot_rows*0.7))\n",
    "    # Plot !\n",
    "    vmin = np.min(data)\n",
    "    vmax = np.max(data)\n",
    "    print vmin,vmax\n",
    "    for i in range(n_kernels):\n",
    "        plt.subplot(plot_rows, plot_cols, i+1)\n",
    "        plt.imshow(data[i], interpolation='none', vmin=vmin, vmax=vmax)\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()\n",
    "        \n",
    "def plot_deepliver_log(fname):\n",
    "    \"\"\"Takes file handle of deepliver log, and plots the 4 plots :\n",
    "    Loss, avgAccuracy, avgJaccard, avgRecall\"\"\"\n",
    "    f = open(fname, 'r')\n",
    "    logs = f.read()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    # Get iterations\n",
    "    iterations = re.findall(\"Iteration (\\d+), loss\",logs)\n",
    "\n",
    "    # Get&plot loss\n",
    "    loss = zip(*re.findall(\"Iteration \\d+, loss = ([+-]?(\\d+(\\.\\d*)?|\\.\\d+)([eE][+-]?\\d+)?)\",logs))[0]\n",
    "    length = min(len(iterations), len(loss))\n",
    "    iterations_trunc, loss_trunc = iterations[:length], loss[:length]\n",
    "    plt.plot(iterations,loss,label='Loss')\n",
    "    #plt.show()\n",
    "    #Get&plot metrics\n",
    "    metrics = ['Accuracy','Recall','Jaccard']\n",
    "    data = defaultdict(list) # data.keys() = metrics , data[metrics[0]] = list of values\n",
    "    for i,metric in enumerate(metrics):\n",
    "        regex = \"Train net output #\"+str(i)+\": accuracy = ([+-]?(\\d+(\\.\\d*)?|\\.\\d+)([eE][+-]?\\d+)?)\"\n",
    "        for result in re.findall(regex,logs):\n",
    "            data[metric].append(result[0])\n",
    "\n",
    "    for metric in data.keys():\n",
    "        length = min(len(iterations),len(data[metric]))\n",
    "        iterations_trunc, data_trunc = iterations[:length], data[metric][:length]\n",
    "        plt.plot(iterations_trunc, data_trunc,label=metric)\n",
    "        plt.legend(loc=\"lower center\",prop={'size':15})\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def histeq(im,nbr_bins=256):\n",
    "    \"\"\"Histogram equalization\"\"\"\n",
    "    #get image histogram\n",
    "    imhist,bins = np.histogram(im.flatten(),nbr_bins,normed=True)\n",
    "    cdf = imhist.cumsum() #cumulative distribution function\n",
    "    cdf = 255 * cdf / cdf[-1] #normalize\n",
    "    #use linear interpolation of cdf to find new pixel values\n",
    "    im2 = np.interp(im.flatten(),bins[:-1],cdf)\n",
    "    return im2.reshape(im.shape)\n",
    "\n",
    "def imshow_overlay_segmentation(him,img,seg,pred):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.title(\"Image\")\n",
    "    plt.imshow(himg)\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.title(\"Ground Truth\")\n",
    "    plt.imshow(img); plt.hold(True)\n",
    "    plt.imshow(seg, cmap=\"Blues\", alpha=0.3)\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.title(\"Prediction\")\n",
    "    plt.imshow(img); plt.hold(True)\n",
    "    plt.imshow(pred, cmap=\"Reds\", alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Javascript for endless cell size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import caffe\n",
    "print caffe.__file__\n",
    "caffe.set_mode_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Params and NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cwd=os.getcwd()\n",
    "basePath='/media/nas/03_Users/01_patrickchrist/vnet'\n",
    "params = dict()\n",
    "params['DataManagerParams']=dict()\n",
    "params['ModelParams']=dict()\n",
    "params['Augmentation']=dict()\n",
    "\n",
    "#params of the algorithm\n",
    "params['ModelParams']['numcontrolpoints']=2\n",
    "params['ModelParams']['sigma']=15\n",
    "params['ModelParams']['device']=0\n",
    "params['ModelParams']['prototxtTrain']=os.path.join(cwd,'Prototxt/train_noPooling_ResNet_cinque.prototxt')\n",
    "params['ModelParams']['prototxtTest']=os.path.join(cwd,'Prototxt/test_noPooling_ResNet_cinque.prototxt')\n",
    "params['ModelParams']['snapshot']=0\n",
    "#params['ModelParams']['dirTrain']='/data/3dircad'\n",
    "params['ModelParams']['dirTrain']='/data/3dircad/test'\n",
    "params['ModelParams']['dirTest']='/data/3dircad'\n",
    "#params['ModelParams']['dirTest']='/data/3dircad/test'\n",
    "params['ModelParams']['dirResult']=os.path.join(basePath,'results') #where we need to save the results (relative to the base path)\n",
    "params['ModelParams']['dirSnapshots']=os.path.join(basePath,'models/3dircad/') #where to save the models while training\n",
    "params['ModelParams']['batchsize'] = 1 #the batchsize\n",
    "params['ModelParams']['numIterations'] = 30000 #the number of iterations\n",
    "params['ModelParams']['baseLR'] = 0.00001 #the learning rate, initial one\n",
    "params['ModelParams']['nProc'] = 6 #the number of threads to do data augmentation\n",
    "\n",
    "\n",
    "#params of the DataManager\n",
    "params['DataManagerParams']['dstRes'] = np.asarray([4,4,3],dtype=float)\n",
    "params['DataManagerParams']['VolSize'] = np.asarray([128,128,64],dtype=int)\n",
    "params['DataManagerParams']['normDir'] = False #if rotates the volume according to its transformation in the mhd file. Not reccommended.\n",
    "params['DataManagerParams']['label_num'] = 2 # Set label to be loaded from GT, e.g. 1 liver 2 lesion\n",
    "params['DataManagerParams']['clipping'] = False\n",
    "params['DataManagerParams']['c_min'] = -100\n",
    "params['DataManagerParams']['c_max'] = 200\n",
    "\n",
    "# Augmentation\n",
    "params['Augmentation']['cascade'] = False\n",
    "params['Augmentation']['deform_prob'] = 0\n",
    "params['Augmentation']['register_prob'] = 0\n",
    "params['Augmentation']['translate_prob'] = 0\n",
    "model=VN.VNet(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data and Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print params['ModelParams']['dirTrain']\n",
    "\n",
    "#we define here a data manage object\n",
    "dataManagerTrain = DM.DataManager(params['ModelParams']['dirTrain'],\n",
    "                                       params['ModelParams']['dirResult'],\n",
    "                                       params['DataManagerParams'])\n",
    "\n",
    "dataManagerTrain.loadTrainingData() #loads in sitk format\n",
    "\n",
    "howManyImages = len(dataManagerTrain.sitkImages)\n",
    "howManyGT = len(dataManagerTrain.sitkGT)\n",
    "\n",
    "assert howManyGT == howManyImages\n",
    "\n",
    "print \"The dataset has shape: data - \" + str(howManyImages) + \". labels - \" + str(howManyGT)\n",
    "\n",
    "test_interval = 50000\n",
    "# Write a temporary solver text file because pycaffe is stupid\n",
    "with open(\"solver.prototxt\", 'w') as f:\n",
    "    f.write(\"train_net: \\\"\" + params['ModelParams']['prototxtTrain'] + \"\\\" \\n\")\n",
    "    f.write(\"base_lr: \" + str(params['ModelParams']['baseLR']) + \" \\n\")\n",
    "    f.write(\"momentum: 0.99 \\n\")\n",
    "    f.write(\"weight_decay: 0.01 \\n\")\n",
    "    f.write(\"lr_policy: \\\"step\\\" \\n\")\n",
    "    f.write(\"stepsize: 5000 \\n\")\n",
    "    f.write(\"gamma: 0.1 \\n\")\n",
    "    f.write(\"display: 1 \\n\")\n",
    "    f.write(\"snapshot: 500 \\n\")\n",
    "    f.write(\"snapshot_prefix: \\\"\" + params['ModelParams']['dirSnapshots'] + \"\\\" \\n\")\n",
    "    f.write(\"test_iter: 3 \\n\")\n",
    "    f.write(\"test_interval: \" + str(test_interval) + \"\\n\")\n",
    "    f.write(\"test_net: \\\"\" + params['ModelParams']['prototxtTest'] + \"\\\" \\n\")\n",
    "            \n",
    "f.close()\n",
    "solver = caffe.SGDSolver(\"solver.prototxt\")\n",
    "\n",
    "if (params['ModelParams']['snapshot'] > 0):\n",
    "    solver.restore(params['ModelParams']['dirSnapshots'] + \"_iter_\" + str(\n",
    "        params['ModelParams']['snapshot']) + \".solverstate\")\n",
    "\n",
    "numpyImages = dataManagerTrain.getNumpyImages()\n",
    "numpyGT = dataManagerTrain.getNumpyGT()\n",
    "\n",
    "#numpyImages['Case00.mhd']\n",
    "#numpy images is a dictionary that you index in this way (with filenames)\n",
    "\n",
    "#for key in numpyImages:\n",
    "    \n",
    "#    mean = np.mean(numpyImages[key][np.logical_and(numpyImages[key]>0,numpyImages[key]<1)])\n",
    "#    std = np.std(numpyImages[key][np.logical_and(numpyImages[key]>0,numpyImages[key]<1)])\n",
    "\n",
    "#    numpyImages[key]-=mean\n",
    "#    numpyImages[key]/=std\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataManagerTest = DM.DataManager(params['ModelParams']['dirTest'], params['ModelParams']['dirResult'], params['DataManagerParams'])\n",
    "dataManagerTest.loadTestData()\n",
    "\n",
    "\n",
    "numpyImagesTest = dataManagerTest.getNumpyImages()\n",
    "numpyGTTest = dataManagerTest.getNumpyGT()\n",
    "\n",
    "for key in numpyImagesTest:\n",
    "    mean = np.mean(numpyImagesTest[key][np.logical_and(numpyImagesTest[key]>0,numpyImagesTest[key]<1)])\n",
    "    std = np.std(numpyImagesTest[key][np.logical_and(numpyImagesTest[key]>0,numpyImagesTest[key]<1)])\n",
    "\n",
    "    numpyImagesTest[key] -= mean\n",
    "    numpyImagesTest[key] /= std\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#WEIGHTS_FILE = params['ModelParams']['dirSnapshots'] + \"_iter_3000.caffemodel\"\n",
    "WEIGHTS_FILE = \"/media/nas/03_Users/01_patrickchrist/vnet/3dircad-liver-models/3dircad/_iter_18500.caffemodel\"\n",
    "solver.net.copy_from(WEIGHTS_FILE)\n",
    "solver.test_nets[0].copy_from(WEIGHTS_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing for Dataloading and online Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepareDataThread(dataQueue, numpyImages, numpyGT):\n",
    "\n",
    "        nr_iter = params['ModelParams']['numIterations']\n",
    "        batchsize = params['ModelParams']['batchsize']\n",
    "\n",
    "        keysIMG = numpyImages.keys()\n",
    "        keysGT = numpyGT.keys()\n",
    "        \n",
    "\n",
    "        # Create Random pairs of images for augmentation\n",
    "        nr_iter_dataAug = nr_iter*batchsize\n",
    "        np.random.seed()\n",
    "        whichDataList = np.random.randint(len(keysIMG), size=int(nr_iter_dataAug/params['ModelParams']['nProc']))\n",
    "        whichDataForMatchingList = np.random.randint(len(keysIMG), size=int(nr_iter_dataAug/params['ModelParams']['nProc']))\n",
    "        \n",
    "\n",
    "        \n",
    "        for whichData,whichDataForMatching in zip(whichDataList,whichDataForMatchingList):\n",
    "            # Load the random pairs of images\n",
    "            filename, ext = splitext(keysIMG[whichData])\n",
    "            gtname, ext = splitext(keysGT[whichData])\n",
    "            \n",
    "            image_num= re.findall(\"\\d+\",filename)[0]\n",
    "            \n",
    "            \n",
    "            currGtKey = \"label\"+ image_num  + ext\n",
    "            currImgKey = filename + ext\n",
    "            \n",
    "            ImgKeyMatching = keysIMG[whichDataForMatching]\n",
    "\n",
    "            defImg = numpyImages[currImgKey]\n",
    "            defLab = numpyGT[currGtKey]\n",
    "            \n",
    "            # data agugumentation through hist matching across different examples...\n",
    "            defImg = utilities.hist_match(defImg, numpyImages[ImgKeyMatching])\n",
    "            \n",
    "            \n",
    "            # Random deform an image to an tamplate\n",
    "            if(np.random.rand(1)[0]<params['Augmentation']['deform_prob'] ): #do not apply deformations always, just sometimes\n",
    "                defImg, defLab = utilities.produceRandomlyDeformedImage(defImg, defLab,\n",
    "                                    params['ModelParams']['numcontrolpoints'],\n",
    "                                              params['ModelParams']['sigma'])\n",
    "\n",
    "                \n",
    "            # Random register an image to an template\n",
    "            if(np.random.rand(1)[0]<params['Augmentation']['register_prob']): #do not apply deformations always, just sometimes\n",
    "                defImg, defLab = utilities.produceRegisteredImage(defImg,defLab,numpyImages[ImgKeyMatching])\n",
    "            \n",
    "            # Random translation of an image\n",
    "            if(np.random.rand(1)[0]<params['Augmentation']['translate_prob']): #do not apply deformations always, just sometimes\n",
    "                defImg, defLab = utilities.produceRandomlyTranslatedImage(defImg,defLab)\n",
    "\n",
    "            \n",
    "                \n",
    "            weightData = np.zeros_like(defLab,dtype=float)\n",
    "            for label in np.unique(defLab):\n",
    "                weightData[defLab == label] = np.prod(defLab.shape) / np.sum((defLab==label).astype(dtype=np.float32))\n",
    "                \n",
    "\n",
    "                \n",
    "            #Cascade ToDo\n",
    "            if params['Augmentation']['cascade']:\n",
    "                liver_mask=sitk.Cast(sitk.ReadImage(join(params['ModelParams']['dirTrain'], currGtKey))>0.5,sitk.sitkFloat32)\n",
    "                #Get the liver index slices\n",
    "                liver_index_slice=np.nonzero(sitk.GetArrayFromImage(liver_mask))[2]\n",
    "                # Cast zeoros\n",
    "                defImg_tmp=np.zeros_like(defImg)\n",
    "                defLab_tmp=np.zeros_like(defLab)\n",
    "                weightData_tmp=np.zeros_like(weightData)\n",
    "                \n",
    "                defImg_tmp[:,:,liver_index_slice]=defImg[:,:,liver_index_slice]\n",
    "                defLab_tmp[:,:,liver_index_slice]=defLab[:,:,liver_index_slice]\n",
    "                weightData_tmp[:,:,liver_index_slice]=weightData_tmp[:,:,liver_index_slice]\n",
    "                defImg=defImg_tmp\n",
    "                defLab=defLab_tmp\n",
    "                weightData=weightData_tmp\n",
    "                \n",
    "            # Mean and STD\n",
    "            mean = np.mean(defImg[np.logical_and(defImg>0,defImg<1)])\n",
    "            std = np.std(defImg[np.logical_and(defImg>0,defImg<1)])\n",
    "\n",
    "            defImg -= mean\n",
    "            defImg /= std\n",
    "                \n",
    "            dataQueue.put(tuple((defImg,defLab, weightData)))\n",
    "            \n",
    "            \n",
    "dataQueue = Queue(30) #max 50 images in queue\n",
    "dataPreparation = [None] * params['ModelParams']['nProc']\n",
    "\n",
    "#thread creation\n",
    "for proc in range(0,params['ModelParams']['nProc']):\n",
    "    dataPreparation[proc] = Process(target=prepareDataThread, args=(dataQueue, numpyImages, numpyGT))\n",
    "    dataPreparation[proc].daemon = True\n",
    "    dataPreparation[proc].start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Config\n",
    "### Config and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enable_label_2 = False #Set to true when segmenting both liver and lesion (labels=0,1,2)\n",
    "use_label1_redblue = False # use redblue dice plot. Useful when training cascade Step2 \n",
    "LOAD_ARRAYS = False # Load arrays from pickled files\n",
    "\n",
    "if use_label1_redblue:\n",
    "    label1_color_train, label1_color_test = \"blue\", \"red\" \n",
    "else:\n",
    "    label1_color_train, label1_color_test = \"#ADB317\", \"#1C7A34\"\n",
    "\n",
    "\n",
    "PLOT_INTERVAL = 10 # Plot one data point every n iterations\n",
    "dices = [] #dices for label=1\n",
    "dices_2 = [] #dices for label=2\n",
    "dices_test =[] # dices for test\n",
    "losses= []\n",
    "accuracies=[]\n",
    "iterations=[]\n",
    "test_dices=[]\n",
    "test_dices_2=[]\n",
    "test_accuracies=[]\n",
    "iter = 0\n",
    "if LOAD_ARRAYS:\n",
    "    i=                pickle.load(open(\"i.int\",'r'))\n",
    "    dices=            pickle.load(open(\"dices.list\",'r'))\n",
    "    if enable_label_2:\n",
    "        dices_2=          pickle.load(open(\"dices_2.list\",'r'))\n",
    "        test_dices_2 =    pickle.load(open(\"test_dices_2.list\",'r'))\n",
    "    losses=           pickle.load(open(\"losses.list\",'r'))\n",
    "    accuracies=       pickle.load(open(\"accuracies.list\",'r'))\n",
    "    iterations =      pickle.load(open(\"iterations.list\",'r'))\n",
    "    test_dices =      pickle.load(open(\"test_dices.list\",'r'))\n",
    "    test_accuracies = pickle.load(open(\"test_accuracies.list\",'r'))\n",
    "    \n",
    "def smooth_last_n(arr, n=5, ignore=None):\n",
    "    \"\"\"Replaces the last n elements in arr (list) with their average.\"\"\"\n",
    "    subarr = np.array(arr[-n:])\n",
    "    if ignore != None:\n",
    "        subarr = subarr[subarr != ignore] \n",
    "    mean = np.mean(subarr)\n",
    "    return arr[:-n]+[mean]\n",
    "\n",
    "iteration_times = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nr_iter = params['ModelParams']['numIterations']\n",
    "batchsize = params['ModelParams']['batchsize']\n",
    "\n",
    "batchData = np.zeros((batchsize, 1, params['DataManagerParams']['VolSize'][0], params['DataManagerParams']['VolSize'][1], params['DataManagerParams']['VolSize'][2]), dtype=float)\n",
    "batchLabel = np.zeros((batchsize, 1, params['DataManagerParams']['VolSize'][0], params['DataManagerParams']['VolSize'][1], params['DataManagerParams']['VolSize'][2]), dtype=float)\n",
    "\n",
    "#only used if you do weighted multinomial logistic regression\n",
    "batchWeight = np.zeros((batchsize, 1, params['DataManagerParams']['VolSize'][0],\n",
    "                       params['DataManagerParams']['VolSize'][1],\n",
    "                       params['DataManagerParams']['VolSize'][2]), dtype=float)\n",
    "\n",
    "train_loss = np.zeros(nr_iter)\n",
    "\n",
    "while iter <nr_iter:\n",
    "    iter=iter+1\n",
    "    for i in range(batchsize):\n",
    "        [defImg, defLab, defWeight] = dataQueue.get()\n",
    "        print np.min(defImg), np.max(defImg)\n",
    "        batchData[i, 0, :, :, :] = defImg.astype(dtype=np.float32)\n",
    "        batchLabel[i, 0, :, :, :] = (defLab > 0.8).astype(dtype=np.float32)\n",
    "        batchWeight[i, 0, :, :, :] = defWeight.astype(dtype=np.float32)\n",
    "\n",
    "    solver.net.blobs['data'].data[...] = batchData.astype(dtype=np.float32)\n",
    "    solver.net.blobs['label'].data[...] = batchLabel.astype(dtype=np.float32)\n",
    "    #solver.net.blobs['labelWeight'].data[...] = batchWeight.astype(dtype=np.float32)\n",
    "    #use only if you do softmax with loss\n",
    "\n",
    "\n",
    "\n",
    "    start_ts = time.time()\n",
    "    solver.step(1)\n",
    "    end_ts   = time.time()\n",
    "    iteration_times.append(end_ts-start_ts)\n",
    "\n",
    "    # Get metrics\n",
    "    img = solver.net.blobs['data'].data[0,0]\n",
    "    seg = np.reshape(solver.net.blobs['label'].data[0,0], img.shape)\n",
    "    print seg.shape\n",
    "    pred= np.argmax(solver.net.blobs['softmax_out'].data[0],axis=0).reshape(seg.shape)\n",
    "    dice_score = dice(pred,seg,1)\n",
    "    dice_score_2 = dice(pred,seg,2) if enable_label_2 else 0\n",
    "    accuracy_score = np.sum(seg==pred)*1.0 / seg.size\n",
    "    loss = float(solver.net.blobs['loss'].data)\n",
    "    \n",
    "    #Save metrics values\n",
    "    iterations.append(iter)\n",
    "    dices.append(dice_score if dice_score>-1 else 1)\n",
    "    dices_2.append(dice_score_2 if dice_score_2>-1 else 1)\n",
    "    accuracies.append(accuracy_score)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if iter % PLOT_INTERVAL == 0:\n",
    "        \n",
    "        ## Run test volumes\n",
    "        pred_test = dict()\n",
    "        dices_test =[]\n",
    "\n",
    "        for key in numpyImagesTest:\n",
    "\n",
    "            btch = np.reshape(numpyImagesTest[key],[1,1,numpyImagesTest[key].shape[0],numpyImagesTest[key].shape[1],numpyImagesTest[key].shape[2]])\n",
    "\n",
    "            solver.test_nets[0].blobs['data'].data[...] = btch\n",
    "\n",
    "            solver.test_nets[0].forward()\n",
    "    \n",
    "            pred_tests= solver.test_nets[0].blobs['labelmap']\n",
    "            \n",
    "            image_num_test_key= re.findall(\"\\d+\",key)[0]\n",
    "            currGtKey_test = \"label\"+ image_num_test_key  + \".nii\"\n",
    "\n",
    "            pred_test[key] = np.squeeze(pred_tests.data[0,1,:,:,:])>0.5\n",
    "            test_dice=dice(pred_test[key],numpyGTTest[currGtKey_test],1)\n",
    "            dices_test.append(test_dice)\n",
    "        \n",
    "\n",
    "        test_dices.append(np.mean(dices_test))\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "        # Print timing stats\n",
    "        avg_iteration_time = np.mean(iteration_times)\n",
    "        iteration_times = []\n",
    "        \n",
    "        liver_train_dices = []\n",
    "            \n",
    "        \n",
    "        \n",
    "        # Smooth\n",
    "        iterations = smooth_last_n(iterations  ,n=PLOT_INTERVAL)\n",
    "        losses     = smooth_last_n(losses      ,n=PLOT_INTERVAL)\n",
    "        dices      = smooth_last_n(dices       ,n=PLOT_INTERVAL)\n",
    "        dices_2    = smooth_last_n(dices_2     ,n=PLOT_INTERVAL) if enable_label_2 else []\n",
    "        accuracies = smooth_last_n(accuracies  ,n=PLOT_INTERVAL)\n",
    "        #test_dices      = smooth_last_n(test_dices       ,n=PLOT_INTERVAL)\n",
    "        \n",
    "        # Print last metrics\n",
    "        print \"Average solver.step duration is\", avg_iteration_time\n",
    "        print 'Loss',losses[-1]\n",
    "        print '#### ACCURACY ####'\n",
    "        print 'Train Accuracy', accuracies[-1]\n",
    "\n",
    "        print \"#### DICE ####\"\n",
    "        print 'Train dice (label=1)',dices[-1]\n",
    "        print \"Test dice (label=2)\", test_dices[-1]\n",
    "        print \"Test dices (Label=2)\", dices_test\n",
    "\n",
    "        if enable_label_2:\n",
    "            print 'Train dice (label=2)',dices_2[-1]\n",
    "\n",
    "        print '\\n'\n",
    "        \n",
    "        # Plot\n",
    "        fig, ax1=plt.subplots()\n",
    "        ax2=ax1.twinx()\n",
    "        ax1.set_xlabel(\"Iterations\")\n",
    "        ax2.set_ylabel(\"Dice\")\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "        ax2.plot(iterations, dices, label=\"Train Dice - Label=1\", color=label1_color_train); plt.hold(True) #dark yellow\n",
    "        ax2.plot(iterations, test_dices, label=\"Test Dice - Label=1\", color=label1_color_test); plt.hold(True)\n",
    "        ax1.plot(iterations, losses, label=\"Loss\", color=\"black\"); plt.hold(True)\n",
    "        leg1 = ax2.legend(loc=\"upper left\", bbox_to_anchor=(1.15,1))\n",
    "        leg2 = ax1.legend(loc=\"upper left\", bbox_to_anchor=(1.15,0.5))\n",
    "        # Make legend clearer\n",
    "        for leghandle in leg1.legendHandles+leg2.legendHandles: leghandle.set_linewidth(10.0)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        if enable_label_2:\n",
    "            fig, ax1=plt.subplots()\n",
    "            ax2=ax1.twinx()\n",
    "            ax1.set_xlabel(\"Iterations\")\n",
    "            ax2.set_ylabel(\"Dice\")\n",
    "            ax1.set_ylabel(\"Loss\")\n",
    "            ax2.plot(iterations, dices_2, label=\"Train Dice - Label=2\",color=\"blue\"); plt.hold(True) #purple #BC23C4\n",
    "            ax1.plot(iterations, losses, label=\"Loss\", color=\"black\"); plt.hold(True)\n",
    "            leg1 = ax2.legend(loc=\"upper left\", bbox_to_anchor=(1.15,1))\n",
    "            leg2 = ax1.legend(loc=\"upper left\", bbox_to_anchor=(1.15,0.5))\n",
    "            # Make legend clearer\n",
    "            for leghandle in leg1.legendHandles+leg2.legendHandles: leghandle.set_linewidth(10.0)\n",
    "            plt.show()\n",
    "\n",
    "        print \"Iteration:\" + str(iter)\n",
    "        print 'Train accuracy on last image :', np.sum(pred==seg)*1.0/pred.size\n",
    "        print 'Train dice Label=1 on last image : ', dice(pred,seg,1)\n",
    "        print 'Test dice Label=1 on last image : ', test_dice\n",
    "        if enable_label_2:\n",
    "            print 'Train dice Label=2 on last image : ', dice(pred,seg,2)\n",
    "        for slice in np.unique(np.nonzero(seg)[2]):\n",
    "            imshow(img[:,:,slice], seg[:,:,slice], pred[:,:,slice], title=[\"Train Image\", \"Ground truth\", \"Prediction\"])\n",
    "        print \"Plotting Test Volumes\"\n",
    "        last_test_key_gt=numpyGTTest.keys()[np.random.randint(len(numpyGTTest.keys()))]\n",
    "        image_num_test= re.findall(\"\\d+\",last_test_key_gt)[0]\n",
    "        last_test_key_image = \"image\"+ image_num_test  + \".nii\"\n",
    "        \n",
    "        for slice_test in np.unique(np.nonzero(numpyGTTest[last_test_key_gt])[2]):\n",
    "            imshow(numpyImagesTest[last_test_key_image][:,:,slice_test], numpyGTTest[last_test_key_gt][:,:,slice_test], pred_test[last_test_key_image][:,:,slice_test], title=[\"Test Image\", \"Ground truth\", \"Prediction\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To resume run this ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(iterations),len(dices),len(dices_2),len(losses),len(accuracies),len(iterations),len(test_dices),len(test_dices_2),len(test_accuracies)\n",
    "if not enable_label_2:\n",
    "    test_dices_2 = test_dices\n",
    "    dices_2 = dices\n",
    "#min_len=min(len(iterations),len(dices),len(dices_2),len(losses),len(accuracies),len(iterations),len(test_dices),len(test_dices_2),len(test_accuracies))\n",
    "min_len=min(len(iterations),len(dices),len(losses),len(accuracies),len(iterations),len(test_dices))\n",
    "print \"Min len\",min_len\n",
    "print 'i',i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data agugumentation through hist matching across different examples...\n",
    "ImgKeyMatching = 'image06.nii'\n",
    "currImgKey='image08.nii'\n",
    "currGtKey='label08.nii'\n",
    "defImg = numpyImages[currImgKey]\n",
    "defLab = numpyGT[currGtKey]\n",
    "\n",
    "\n",
    "defImg = utilities.hist_match(defImg, numpyImages[ImgKeyMatching])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Random deform an image to an tamplate\n",
    "if(np.random.rand(1)[0]<params['Augmentation']['deform_prob'] ): #do not apply deformations always, just sometimes\n",
    "    defImg, defLab = utilities.produceRandomlyDeformedImage(defImg, defLab,\n",
    "                        params['ModelParams']['numcontrolpoints'],\n",
    "                                  params['ModelParams']['sigma'])\n",
    "\n",
    "\n",
    "# Random register an image to an template\n",
    "if(np.random.rand(1)[0]<params['Augmentation']['register_prob']): #do not apply deformations always, just sometimes\n",
    "    defImg, defLab = utilities.produceRegisteredImage(defImg,defLab,numpyImages[ImgKeyMatching])\n",
    "\n",
    "# Random translation of an image\n",
    "if(np.random.rand(1)[0]<params['Augmentation']['translate_prob']): #do not apply deformations always, just sometimes\n",
    "    defImg, defLab = utilities.produceRandomlyTranslatedImage(defImg,defLab)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numpyImages[ImgKeyMatching].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numpyGTTest.keys()[np.random.randint(len(numpyGTTest.keys()))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### then this ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In case of resumed training, make sure all lists have equal size. Since kernel interruption might cause them to be \n",
    "# not equal\n",
    "#n_ignored_entries = min_len%PLOT_INTERVAL\n",
    "#min_len -= n_ignored_entries\n",
    "if True:\n",
    "    n_ignored_entries = len(dices) - len(test_dices)\n",
    "    #min_len = len(dices) - n_ignored_entries\n",
    "    dices = dices[:min_len]\n",
    "    dices_2=dices_2[:min_len]\n",
    "    losses= losses[:min_len]\n",
    "    accuracies=accuracies[:min_len]\n",
    "    iterations=iterations[:min_len]\n",
    "    test_dices=test_dices[:min_len]\n",
    "    test_dices_2=test_dices_2[:min_len]\n",
    "    test_accuracies=test_accuracies[:min_len]\n",
    "    i = len(dices) * PLOT_INTERVAL\n",
    "\n",
    "print len(iterations),len(dices),len(dices_2),len(losses),len(accuracies),len(iterations),len(test_dices),len(test_accuracies)\n",
    "print 'i',i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# In case of resumed training, make sure all lists have equal size. Since kernel interruption might cause them to be \n",
    "# not equal\n",
    "n_ignored_entries = min_len%PLOT_INTERVAL\n",
    "min_len -= n_ignored_entries\n",
    "\n",
    "\n",
    "print len(iterations),len(dices),len(dices_2),len(losses),len(accuracies),len(iterations),len(test_dices),len(test_accuracies)\n",
    "print 'i',i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## ---- End of training notebook (the rest is one-off analysis) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "defImg, defLab = utilities.produceRegisteredImage(defImg,defLab,numpyImages[ImgKeyMatching])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blobs=solver.net.blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "td2pure = td2[np.logical_and(td2!=1.0, td2!=0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.count_nonzero(test_dices_2==1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fidx=1\n",
    "layer_name='u0c'\n",
    "for fidx in range(blobs[layer_name].data.shape[1]):\n",
    "    last_layer_img = blobs[layer_name].data[0,fidx, :,:]\n",
    "    imshow(last_layer_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in iterations:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lyr=solver.net.layers[64]\n",
    "lyr.type\n",
    "lyr.blobs.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for param_name in solver.net.params:\n",
    "    print param_name,\"\\t\",solver.net.params[param_name][0].data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_kernels(solver.net.params[\"conv_d0a-b\"][0].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx in range(blobs['d2c'].data.shape[1]):\n",
    "    imshow(blobs['d2c'].data[0,idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_kernels(blobs['d0b'].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blobs['data'].data[0,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blobs['label'].data[0,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print solver.net.params[\"conv_d0a-b\"][0].data.shape\n",
    "imshow(solver.net.params[\"conv_d0a-b\"][0].data[7,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imshow(blobs['data'].data[0,0][:,:,23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "imshow(blobs['label'].data[0,0][:,:,23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.signal import convolve2d\n",
    "filter_idx = 7\n",
    "image = blobs['data'].data[0,0]\n",
    "bias = solver.net.params[\"conv_d0a-b\"][1].data[filter_idx]\n",
    "kernel = solver.net.params[\"conv_d0a-b\"][0].data[filter_idx,0]\n",
    "result = convolve2d(image, kernel) + bias\n",
    "print bias\n",
    "imshow(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.signal.convolve2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print blobs['d0b'].data.shape\n",
    "imshow(blobs['d0b'].data[0,15],blobs['d0b'].data[0,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get iteration with best test dice ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dice_iter = zip(test_dices,iterations)\n",
    "dice_iter = sorted(dice_iter, key=lambda t:t[0], reverse=True)\n",
    "for ji in range(10):\n",
    "    print str(ji+1)+'th best test Dice:\\t',round(dice_iter[ji][0],3),'\\tAt iteration:\\t',dice_iter[ji][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save plots\n",
    "import pickle\n",
    "pickle.dump(i, open(\"i.int\",'w'))\n",
    "pickle.dump(dices, open(\"dices.list\",'w'))\n",
    "pickle.dump(dices_2, open(\"dices_2.list\",'w'))\n",
    "pickle.dump(losses, open(\"losses.list\",'w'))\n",
    "pickle.dump(accuracies, open(\"accuracies.list\",'w'))\n",
    "pickle.dump(iterations, open(\"iterations.list\",'w'))\n",
    "pickle.dump(test_dices, open(\"test_dices.list\",'w'))\n",
    "pickle.dump(test_dices_2, open(\"test_dices_2.list\",'w'))\n",
    "pickle.dump(test_accuracies, open(\"test_accuracies.list\",'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean = protobinary_to_array(\"mean.protobinary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Training examples ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    solver.net.forward()\n",
    "    img = (blobs['data'].data[0,0]+mean)[92:480,92:480]\n",
    "    himg =histeq(img)\n",
    "    seg = blobs['label'].data[0,0]\n",
    "    pred= np.argmax(blobs['score'].data[0],axis=0)\n",
    "\n",
    "    imshow_overlay_segmentation(himg,img,seg,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict TEST examples #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    solver.test_nets[0].forward()\n",
    "    img = (testblobs['data'].data[0,0]+mean)[92:480,92:480]\n",
    "    himg =histeq(img)\n",
    "    seg = testblobs['label'].data[0,0]\n",
    "    pred= np.argmax(testblobs['score'].data[0],axis=0)\n",
    "\n",
    "    imshow_overlay_segmentation(himg,img,seg,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avg Dice score over slices #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dices_lesions_ = []\n",
    "for _ in range(1000):\n",
    "    solver.test_nets[0].forward()\n",
    "    seg = solver.test_nets[0].blobs['label'].data[0,0]\n",
    "    pred = solver.test_nets[0].blobs['score'].data[0].argmax(0)\n",
    "    dice_lesion_ = dice(pred,seg,label_of_interest=1)\n",
    "\n",
    "    if(dice_lesion_ > -1):\n",
    "        dices_lesions_.append(dice_lesion_)\n",
    "    print \"Average TEST dice lesion: \", np.average(dices_lesions_)\n",
    "\n",
    "print \"FINAL Average TEST dice lesion: \", np.average(dices_lesions_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avg Dice score over individual Lesions #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import scipy.spatial.distance\n",
    "import scipy.ndimage\n",
    "import scipy.ndimage.measurements\n",
    "from collections import defaultdict\n",
    "def dice_separate_lesions(seg,pred, plot=False):\n",
    "    \"\"\"Returns Avg dice of lesion structures and weight to assign to this avg dice.\"\"\"\n",
    "    #Ignore liver\n",
    "    if np.unique(seg).size > 2:\n",
    "        seg[seg==1] = 0\n",
    "        seg[seg==2] = 1\n",
    "    if np.unique(pred).size > 2:\n",
    "        pred[pred==1] = 0\n",
    "        pred[pred==2] = 1\n",
    "    # First component is always background\n",
    "    seg[0,0] = 0\n",
    "    pred[0,0] = 0\n",
    "    # Get connected components\n",
    "    comps_seg, num_comps_seg = scipy.ndimage.label(seg)\n",
    "    comps_pred, num_comps_pred = scipy.ndimage.label(pred)\n",
    "    #print 'Found n connected components in ground truth (not including bg) :', num_comps_seg\n",
    "    if plot: imshow(comps_seg, comps_pred, cmap=\"Spectral\", title=['Components in Ground Truth','Components in Prediction'])\n",
    "    # Get component centroids\n",
    "    centroids_seg = scipy.ndimage.measurements.center_of_mass(seg, comps_seg, range(1, num_comps_seg+1))\n",
    "    centroids_pred = scipy.ndimage.measurements.center_of_mass(pred, comps_pred, range(1, num_comps_pred+1))\n",
    "    # round to nearest 2 decimals (otherwise we might have problems removing from list by-value due to fp inaccuracies)\n",
    "    centroids_seg = map(lambda t:(round(t[0],2), round(t[1],2)), centroids_seg)\n",
    "    centroids_pred = map(lambda t:(round(t[0],2), round(t[1],2)), centroids_pred)\n",
    "    \n",
    "    def plot_centroids(comps_img, centroids, title, w=5):\n",
    "        centroid_img = np.ones(comps_img.shape)\n",
    "        for x,y in centroids:\n",
    "            centroid_img[x-w:x+w, y-w:y+w] = 0\n",
    "        plt.title(title)\n",
    "        plt.imshow(comps_img, cmap=\"Spectral\"); plt.hold(True)\n",
    "        plt.imshow(centroid_img,cmap=\"Reds\",alpha=0.5)\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    if plot: plot_centroids(comps_seg, centroids_seg, \"Centroids in Ground Truth\")\n",
    "    if plot: plot_centroids(comps_pred, centroids_pred, \"Centroids in Prediction\")\n",
    "    \n",
    "    #### Get Average dice ####\n",
    "    def get_closest(xy, list_xy, except_at_idx):\n",
    "        \"\"\"Returns the index of coordinate in list_xy that is closest to xy (euclidean distance)\n",
    "        example: get_closest((100,100), [(3,4), (5,9), (101,102), (9999,9999)]) = 2\n",
    "        because (101,102) is the closest to (100,100).\n",
    "        except_at_idx is a list of coordinate indices to ignore in list_xy\"\"\"\n",
    "        closest_idx = -1\n",
    "        min_dist = sys.maxint\n",
    "        for i, xy_dest in enumerate(list_xy):\n",
    "            if i in except_at_idx:\n",
    "                continue\n",
    "            dist = scipy.spatial.distance.euclidean(xy, xy_dest)\n",
    "            if dist < min_dist:\n",
    "                closest_idx = i\n",
    "                min_dist = dist\n",
    "        #print xy, map(lambda t:(round(t[0]),round(t[1])),list_xy), closest_idx\n",
    "        return closest_idx\n",
    "    \n",
    "    dices = []\n",
    "    consumed_lesions_idx = [] #indices of lesions already consumed.\n",
    "    # Iterate after bg component\n",
    "    for i in range(num_comps_pred):\n",
    "        # Add 0 dice to false positives!\n",
    "        if len(centroids_seg) == 0:\n",
    "            dices.append(0)\n",
    "            continue\n",
    "        current_xy = centroids_pred[i]\n",
    "        closest_component = get_closest(current_xy, centroids_seg, except_at_idx=consumed_lesions_idx)\n",
    "        consumed_lesions_idx.append(closest_component)\n",
    "        #mask out other components \n",
    "        one_lesion_pred = np.clip(comps_pred == i, 0, 1)\n",
    "        one_lesion_seg = np.clip(comps_seg == closest_component, 0, 1)\n",
    "        dices.append(dice(one_lesion_pred, one_lesion_seg, label_of_interest = 1))\n",
    "    \n",
    "    # Add 0 dice for false negatives\n",
    "    if len(centroids_seg)-len(consumed_lesions_idx) > 0:\n",
    "        dices.extend([0]*(len(centroids_seg)-len(consumed_lesions_idx)))\n",
    "\n",
    "    return np.mean(dices), len(dices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dices_lesions_ = []\n",
    "weights = []\n",
    "for _ in range(200):\n",
    "    solver.test_nets[0].forward()\n",
    "    seg = solver.test_nets[0].blobs['label'].data[0,0]\n",
    "    pred = solver.test_nets[0].blobs['score'].data[0].argmax(0)\n",
    "    dice_lesion_,weight = dice_separate_lesions(seg,pred)\n",
    "\n",
    "    if(dice_lesion_ > -1):\n",
    "        dices_lesions_.append(dice_lesion_)\n",
    "        weights.append(weight)\n",
    "\n",
    "total = np.multiply(dices_lesions_, weights)\n",
    "print \"Average TEST dice lesion: \", np.average(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing threshold (instead of 0.5) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solver.test_nets[0].forward()\n",
    "img = (testblobs['data'].data[0,0]+mean)[92:480,92:480]\n",
    "himg =histeq(img)\n",
    "seg = testblobs['label'].data[0,0]\n",
    "pred= np.argmax(testblobs['score'].data[0],axis=0)\n",
    "imshow_overlay_segmentation(himg,img,seg,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prob = testblobs['prob'].data[0,1]\n",
    "imshow(prob)\n",
    "pred_t = prob>0.7\n",
    "print dice(seg,pred)\n",
    "print dice(seg,pred_t)\n",
    "imshow(pred_t,seg)\n",
    "#prob[np.logical_and(prob>0.4, prob<0.6)].size*1.0/prob.size\n",
    "\n",
    "\n",
    "#imshow_overlay_segmentation(himg,img,seg,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax(a1,a2):\n",
    "    s1 = np.exp(a1)\n",
    "    s2 = np.exp(a2)\n",
    "    sm= s1+s2\n",
    "    return s1/sm , s2/sm\n",
    "\n",
    "thresholds = np.linspace(0,1,40) #20 thresholds\n",
    "dices_athalf = []\n",
    "dices_bythreshold = defaultdict(list) # {0.5:[list of dices], 0.6:[list of dices]}\n",
    "for _ in range(2000):\n",
    "    solver.net.forward()\n",
    "    img = (blobs['data'].data[0,0]+mean)[92:480,92:480]\n",
    "    himg =histeq(img)\n",
    "    seg = blobs['label'].data[0,0]\n",
    "    prob = softmax(blobs['score'].data[0,0],blobs['score'].data[0,1])[1] #probability being a lesion\n",
    "    dices_athalf.append(dice(seg,prob>0.5))\n",
    "    for t in thresholds :\n",
    "        pred_t = prob > t\n",
    "        dice_score = dice(seg,pred_t)\n",
    "        dices_bythreshold[t].append(dice_score)\n",
    "        \n",
    "# Aggregate dices over slices for each threshold\n",
    "avgdices = []\n",
    "for t in thresholds:\n",
    "    avgdices.append(np.average(dices_bythreshold[t]))\n",
    "\n",
    "plt.plot(thresholds,avgdices)\n",
    "print 'Found max dice at threshold :', thresholds[np.argmax(avgdices)]\n",
    "print 'Found max dice score :', np.max(avgdices)\n",
    "print 'Vs. the dice at 0.5 which equals :', np.average(dices_athalf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "THRESHOLD = 0.820512820513\n",
    "dices = []\n",
    "for _ in range(2000):\n",
    "    solver.test_nets[0].forward()\n",
    "    img = (testblobs['data'].data[0,0]+mean)[92:480,92:480]\n",
    "    seg = testblobs['label'].data[0,0]\n",
    "    prob = testblobs['prob'].data[0,1] #probability being a lesion\n",
    "    dice_score = dice(seg,prob> THRESHOLD)\n",
    "    dices.append(dice_score)\n",
    "    \n",
    "print 'Average TEST threshold :', np.average(dices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dices = []\n",
    "for _ in range(2000):\n",
    "    solver.test_nets[0].forward()\n",
    "    img = (testblobs['data'].data[0,0]+mean)[92:480,92:480]\n",
    "    seg = testblobs['label'].data[0,0]\n",
    "    pred = np.argmax(testblobs['prob'].data[0], axis=0) #probability being a lesion\n",
    "    dice_score = dice(seg,pred)\n",
    "    dices.append(dice_score)\n",
    "    \n",
    "print 'Average TEST threshold :', np.average(dices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0,1,40) #20 thresholds\n",
    "dices_athalf = []\n",
    "dices_bythreshold = defaultdict(list) # {0.5:[list of dices], 0.6:[list of dices]}\n",
    "for _ in range(2000):\n",
    "    solver.test_nets[0].forward()\n",
    "    img = (testblobs['data'].data[0,0]+mean)[92:480,92:480]\n",
    "    himg =histeq(img)\n",
    "    seg = testblobs['label'].data[0,0]\n",
    "    prob = testblobs['prob'].data[0,1] #probability being a lesion\n",
    "    dices_athalf.append(dice(seg,prob>0.5))\n",
    "    for t in thresholds :\n",
    "        pred_t = prob > t\n",
    "        dice_score = dice(seg,pred_t)\n",
    "        dices_bythreshold[t].append(dice_score)\n",
    "        \n",
    "# Aggregate dices over slices for each threshold\n",
    "avgdices = []\n",
    "for t in thresholds:\n",
    "    avgdices.append(np.average(dices_bythreshold[t]))\n",
    "\n",
    "plt.plot(thresholds,avgdices)\n",
    "print 'Found max dice at threshold :', thresholds[np.argmax(avgdices)]\n",
    "print 'Found max dice score :', np.max(avgdices)\n",
    "print 'Vs. the dice at 0.5 which equals :', np.average(dices_athalf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototxts #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat solver_unet.prototxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat unet-overfit.prototxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solver.net.blobs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
